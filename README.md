# ğŸš€ Data Pipeline ELT (Node.js + PostgreSQL)

A structured end-to-end ELT data pipeline built with **Node.js** and **PostgreSQL (Neon Cloud)**.
This project is part of my hands-on data engineering learning journey, focused on building a real pipeline architecture instead of just writing queries.

Although this is a practice project, it follows real-world pipeline structure and engineering patterns.

---

## ğŸ¯ Project Goal

To design and implement a simple but structured ELT pipeline that demonstrates:

- Data extraction from an external API
- Staging layer pattern
- Data transformation using SQL
- Loading into a dimensional table
- Transaction handling
- Logging
- Scheduled automation using GitHub Actions

The goal is to build strong foundations in data engineering concepts through practical implementation.

---

## ğŸ—ï¸ Architecture Overview

```
External API
      â†“
Extract (Node.js)
      â†“
Staging Table (PostgreSQL)
      â†“
Transform (SQL)
      â†“
Dimensional Table (dim_users)
```

- Database: PostgreSQL (Neon Cloud)
- Runtime: Node.js
- Automation: GitHub Actions (scheduled cron)
- Logging: Winston

---

## ğŸ› ï¸ Tech Stack

- Node.js
- PostgreSQL (Neon)
- GitHub Actions (CI/CD scheduling)
- Winston (structured logging)
- dotenv (environment configuration)

---

## ğŸ“‚ Project Structure

```
data_pipeline_elt/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ db.js
â”‚   â”œâ”€â”€ extract.js
â”‚   â”œâ”€â”€ loader.js
â”‚   â”œâ”€â”€ transform.js
â”‚   â””â”€â”€ logger.js
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ pipeline.yml
â”‚
â”œâ”€â”€ index.js
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline Flow

### 1ï¸âƒ£ Schema Initialization

Ensures required tables exist:

- `stg_users` (raw staging data)
- `dim_users` (clean dimensional table)

### 2ï¸âƒ£ Extract

Fetches user data from an external API.

### 3ï¸âƒ£ Load (Staging Layer)

Inserts raw data into the staging table.

### 4ï¸âƒ£ Transform & Load

- Cleans and filters data
- Uses SQL-based transformation
- Applies `ON CONFLICT` upsert logic to maintain idempotency

All critical operations are wrapped inside database transactions (`BEGIN / COMMIT / ROLLBACK`) to ensure consistency.

---

## â° Automation

The pipeline runs automatically via **GitHub Actions** on a daily schedule:

```
02:00 UTC
```

Database credentials are stored securely using GitHub Secrets.

---

## âœ… Current Features

- Structured ELT architecture
- Staging-to-dimension pattern
- Idempotent upsert logic
- Transaction handling
- Logging (console + file)
- Scheduled CI/CD pipeline
- Cloud database integration (Neon)

---

## ğŸ“ˆ What This Project Demonstrates

This project reflects practical understanding of:

- End-to-end data pipeline design
- Separation of extract, load, and transform logic
- Safe database operations
- Basic dimensional modeling
- Automation & deployment workflow

It is intentionally simple but structured in a way that can be extended toward production-grade patterns.

---

## ğŸš§ Planned Improvements

To continue leveling up this project:

- Incremental loading (watermark-based)
- Bulk insert optimization
- Retry & exponential backoff for API calls
- Pipeline metadata tracking (run history)
- Slowly Changing Dimension (SCD Type 2)
- Data validation layer
- Performance tuning & indexing

---

## âš™ï¸ How to Run Locally

1. Clone the repository:

```
git clone https://github.com/asrorymous/data_pipeline_elt.git
```

2. Install dependencies:

```
npm install
```

3. Create a `.env` file:

```
DATABASE_URL=your_neon_connection_string
```

4. Run the pipeline:

```
npm start
```

---

## ğŸ‘¨â€ğŸ’» About This Project

This project is part of my continuous learning in data engineering.
The focus is not only on making the pipeline run, but on understanding structure, reliability, and scalability step by step.

It is designed to evolve as I deepen my knowledge in modeling, performance optimization, and production reliability.

---
